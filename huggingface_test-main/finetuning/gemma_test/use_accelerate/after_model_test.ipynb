{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d83a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8de418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"google/gemma-2b-it\"\n",
    "ADAPTER_MODEL = \"outputs/checkpoint-50\"  # 학습 후 모델저장 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53de8e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb4d7f",
   "metadata": {},
   "source": [
    "## 학습 전 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9ded1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b9e34b1e0e44ceadfa700ef86807d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_origin = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c602c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "생성 중...\n",
      "\n",
      "=== 전체 출력 ===\n",
      "<bos><start_of_turn>user\n",
      "다음 글을 요약해주세요:\n",
      "\n",
      "서울시는 29일 2025년 예산안을 발표했다. 총 예산 규모는 50조원으로 \n",
      "전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 \n",
      "복지 정책 강화다. 시는 지하철 노선 확장과 저소득층 지원 예산을 \n",
      "대폭 늘렸다고 밝혔다.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "* 서울시는 2025년 예산안을 발표했다.\n",
      "* 총 예산 규모는 50조원으로 5% 증가했다.\n",
      "* 주요 투자 분야는 교통 인프라 확충과 복지 정책 강화다.\n",
      "* 시는 지하철 노선 확장과 저소득층 지원 예산을 늘렸다고 밝혔다.<eos>\n",
      "\n",
      "=== 요약 결과 ===\n",
      "* 서울시는 2025년 예산안을 발표했다.\n",
      "* 총 예산 규모는 50조원으로 5% 증가했다.\n",
      "* 주요 투자 분야는 교통 인프라 확충과 복지 정책 강화다.\n",
      "* 시는 지하철 노선 확장과 저소득층 지원 예산을 늘렸다고 밝혔다\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 추론 모드\n",
    "model_origin.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 테스트 문서\n",
    "document = \"\"\"\n",
    "서울시는 29일 2025년 예산안을 발표했다. 총 예산 규모는 50조원으로 \n",
    "전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 \n",
    "복지 정책 강화다. 시는 지하철 노선 확장과 저소득층 지원 예산을 \n",
    "대폭 늘렸다고 밝혔다.\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 생성 (tokenizer가 <bos>를 자동으로 추가하므로 제거)\n",
    "prompt = f\"\"\"<start_of_turn>user\n",
    "다음 글을 요약해주세요:\n",
    "\n",
    "{document.strip()}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# 토크나이징\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(model_origin.parameters()).device)\n",
    "\n",
    "# 생성\n",
    "print(\"\\n생성 중...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model_origin.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.3,  # 더 낮은 temperature로 일관성 향상\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2,  # 반복 방지\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# 결과 출력\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(\"\\n=== 전체 출력 ===\")\n",
    "print(result)\n",
    "\n",
    "# 요약만 추출 (더 정확한 파싱)\n",
    "if \"<start_of_turn>model\" in result:\n",
    "    summary = result.split(\"<start_of_turn>model\")[-1]\n",
    "    # <end_of_turn>이나 <eos> 전까지만 추출\n",
    "    summary = summary.split(\"<end_of_turn>\")[0].split(\"<eos>\")[0].strip()\n",
    "    \n",
    "    # 이상한 패턴 제거\n",
    "    patterns_to_remove = [\"municipi :\", \"qed\", \"QED\", \"Q.E.D.\"]\n",
    "    for pattern in patterns_to_remove:\n",
    "        if pattern in summary:\n",
    "            summary = summary.split(pattern)[0].strip()\n",
    "    \n",
    "    # 끝에 붙은 이상한 단어들 제거\n",
    "    summary = summary.rstrip(\" .\").strip()\n",
    "else:\n",
    "    summary = result.strip()\n",
    "\n",
    "print(\"\\n=== 요약 결과 ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f08b99",
   "metadata": {},
   "source": [
    "## 파인튜닝 후 LoRA 어뎁터 적용 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b3c1d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315d9dc19631493bb2b7400837a8c0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f71ab977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "생성 중...\n",
      "\n",
      "=== 전체 출력 ===\n",
      "<bos><start_of_turn>user\n",
      "다음 글을 요약해주세요:\n",
      "\n",
      "서울시는 29일 2025년 예산안을 발표했다. 총 예산 규모는 50조원으로 \n",
      "전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 \n",
      "복지 정책 강화다. 시는 지하철 노선 확장과 저소득층 지원 예산을 \n",
      "대폭 늘렸다고 밝혔다.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "서울시는 29일 2025년 예산안을 발표했는데, 총 예산 규모는 50조원으로 전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 복지 정책 강화다. 서울시는 지하철 노선 확장과 저소득층 지원 예산을 늘렸다고 밝혔다.municipi : 서울시는 29일 2025년 예산안을 발표했는데, 총 예산 규모는 50조원으로 전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 복지 정책 강화다. 서울시는 지하철 노선 확장과 저소득층 지원 예산을 늘렸다고 밝혔다.municipi : 서울시\n",
      "\n",
      "=== 요약 결과 ===\n",
      "서울시는 29일 2025년 예산안을 발표했는데, 총 예산 규모는 50조원으로 전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 복지 정책 강화다. 서울시는 지하철 노선 확장과 저소득층 지원 예산을 늘렸다고 밝혔다\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 추론 모드\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 테스트 문서\n",
    "document = \"\"\"\n",
    "서울시는 29일 2025년 예산안을 발표했다. 총 예산 규모는 50조원으로 \n",
    "전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 \n",
    "복지 정책 강화다. 시는 지하철 노선 확장과 저소득층 지원 예산을 \n",
    "대폭 늘렸다고 밝혔다.\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 생성 (tokenizer가 <bos>를 자동으로 추가하므로 제거)\n",
    "prompt = f\"\"\"<start_of_turn>user\n",
    "다음 글을 요약해주세요:\n",
    "\n",
    "{document.strip()}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# 토크나이징\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "\n",
    "# 생성\n",
    "print(\"\\n생성 중...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.3,  # 더 낮은 temperature로 일관성 향상\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2,  # 반복 방지\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# 결과 출력\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(\"\\n=== 전체 출력 ===\")\n",
    "print(result)\n",
    "\n",
    "# 요약만 추출 (더 정확한 파싱)\n",
    "if \"<start_of_turn>model\" in result:\n",
    "    summary = result.split(\"<start_of_turn>model\")[-1]\n",
    "    # <end_of_turn>이나 <eos> 전까지만 추출\n",
    "    summary = summary.split(\"<end_of_turn>\")[0].split(\"<eos>\")[0].strip()\n",
    "    \n",
    "    # 이상한 패턴 제거\n",
    "    patterns_to_remove = [\"municipi :\", \"qed\", \"QED\", \"Q.E.D.\"]\n",
    "    for pattern in patterns_to_remove:\n",
    "        if pattern in summary:\n",
    "            summary = summary.split(pattern)[0].strip()\n",
    "    \n",
    "    # 끝에 붙은 이상한 단어들 제거\n",
    "    summary = summary.rstrip(\" .\").strip()\n",
    "else:\n",
    "    summary = result.strip()\n",
    "\n",
    "print(\"\\n=== 요약 결과 ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "699c73f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969303d6e3a34df1ac4db2c07b31ba5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_origin = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d42b0f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8628c284154495684256aa49ce16573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapter...\n",
      "\n",
      "생성 중...\n",
      "\n",
      "=== 전체 출력 ===\n",
      "<bos><start_of_turn>user\n",
      "다음 글을 요약해주세요:\n",
      "\n",
      "서울시는 29일 2025년 예산안을 발표했다. 총 예산 규모는 50조원으로 \n",
      "전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 \n",
      "복지 정책 강화다. 시는 지하철 노선 확장과 저소득층 지원 예산을 \n",
      "대폭 늘렸다고 밝혔다.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "서울시는 29일 2025년 예산안을 발표했는데, 총 예산 규모는 50조원으로 전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 복지 정책 강화다. 서울은 지하철 노선 확장과 저소득층 지원 예산을 대폭 늘렸다고 밝혔다.municipi : 서울시의 2025년 예산안이 발표됐다. seoulnews1.com 김영진 기자 서울시는 지하철 노선 확장과 \n",
      "복지 정책 강화를 위해 29일 총 예산 규모 50조원을 발표했다. 서울시는 지하철 노선 확장과 저소득층 지원 예산을 대폭 늘렸다고 밝혔다. 서울\n",
      "\n",
      "=== 요약 결과 ===\n",
      "서울시는 29일 2025년 예산안을 발표했는데, 총 예산 규모는 50조원으로 전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 복지 정책 강화다. 서울은 지하철 노선 확장과 저소득층 지원 예산을 대폭 늘렸다고 밝혔다\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL)\n",
    "\n",
    "# 추론 모드\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 테스트 문서\n",
    "document = \"\"\"\n",
    "서울시는 29일 2025년 예산안을 발표했다. 총 예산 규모는 50조원으로 \n",
    "전년 대비 5% 증가했다. 주요 투자 분야는 교통 인프라 확충과 \n",
    "복지 정책 강화다. 시는 지하철 노선 확장과 저소득층 지원 예산을 \n",
    "대폭 늘렸다고 밝혔다.\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 생성 (tokenizer가 <bos>를 자동으로 추가하므로 제거)\n",
    "prompt = f\"\"\"<start_of_turn>user\n",
    "다음 글을 요약해주세요:\n",
    "\n",
    "{document.strip()}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# 토크나이징\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "\n",
    "# 생성\n",
    "print(\"\\n생성 중...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.3,  # 더 낮은 temperature로 일관성 향상\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2,  # 반복 방지\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# 결과 출력\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(\"\\n=== 전체 출력 ===\")\n",
    "print(result)\n",
    "\n",
    "# 요약만 추출 (더 정확한 파싱)\n",
    "if \"<start_of_turn>model\" in result:\n",
    "    summary = result.split(\"<start_of_turn>model\")[-1]\n",
    "    # <end_of_turn>이나 <eos> 전까지만 추출\n",
    "    summary = summary.split(\"<end_of_turn>\")[0].split(\"<eos>\")[0].strip()\n",
    "    \n",
    "    # 이상한 패턴 제거\n",
    "    patterns_to_remove = [\"municipi :\", \"qed\", \"QED\", \"Q.E.D.\"]\n",
    "    for pattern in patterns_to_remove:\n",
    "        if pattern in summary:\n",
    "            summary = summary.split(pattern)[0].strip()\n",
    "    \n",
    "    # 끝에 붙은 이상한 단어들 제거\n",
    "    summary = summary.rstrip(\" .\").strip()\n",
    "else:\n",
    "    summary = result.strip()\n",
    "\n",
    "print(\"\\n=== 요약 결과 ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043041c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
